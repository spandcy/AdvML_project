{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NIN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mACz3jTaakTV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42261be3-b42a-4826-db95-4aaedfcd3684"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.initializers import RandomNormal  \n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AjO4I08nakUJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size    = 128\n",
        "epochs        = 150\n",
        "iterations    = 410\n",
        "num_classes   = 10\n",
        "dropout       = 0.5\n",
        "weight_decay  = 0.0002\n",
        "log_filepath  = './nin_bn'\n",
        "#!tensorboard --logdir=/nin_bn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBMu6jflakUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "if('tensorflow' == K.backend()):\n",
        "    import tensorflow as tf\n",
        "    from keras.backend.tensorflow_backend import set_session\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i5ZE_JCxakUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def color_preprocessing(x_train,x_test):\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    mean = [125.307, 122.95, 113.865]\n",
        "    std  = [62.9932, 62.0887, 66.7048]\n",
        "    for i in range(3):\n",
        "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
        "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
        "\n",
        "    return x_train, x_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFIhJ5NNakXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def scheduler(epoch):\n",
        "    if epoch <= 60:\n",
        "        return 0.05\n",
        "    if epoch <= 120:\n",
        "        return 0.01\n",
        "    if epoch <= 160:    \n",
        "        return 0.002\n",
        "    return 0.0004"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iah7R4H2akZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
        "  \n",
        "  model.add(Dropout(dropout))\n",
        "  \n",
        "  model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
        "  \n",
        "  model.add(Dropout(dropout))\n",
        "  \n",
        "  model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  \n",
        "  model.add(GlobalAveragePooling2D())\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v89Nj312akZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8030
        },
        "outputId": "63619a7d-e03f-4acc-fe65-32577dc5eee2"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # load data\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "    \n",
        "    x_train, x_test = color_preprocessing(x_train, x_test)\n",
        "\n",
        "    # build network\n",
        "    model = build_model()\n",
        "    print(model.summary())\n",
        "\n",
        "    # set callback\n",
        "    tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0,  write_graph = True, write_images = False)\n",
        "    change_lr = LearningRateScheduler(scheduler)\n",
        "    cbks = [change_lr,tb_cb]\n",
        "\n",
        "\n",
        "    # set data augmentation\n",
        "    print('Using real-time data augmentation.')\n",
        "    datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # start training\n",
        "    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),steps_per_epoch=iterations,epochs=epochs,callbacks=cbks,validation_data=(x_test, y_test))\n",
        "    model.save('nin_bn.h5')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 991s 6us/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 192)       14592     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 192)       768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 160)       30880     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 160)       640       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 160)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 96)        15456     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 96)        384       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 192)       460992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 192)       37056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 192)       37056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 192)         331968    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 192)         37056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 10)          1930      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 8, 8, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8, 8, 10)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 972,658\n",
            "Trainable params: 969,822\n",
            "Non-trainable params: 2,836\n",
            "_________________________________________________________________\n",
            "None\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/200\n",
            "391/391 [==============================] - 68s 175ms/step - loss: 1.6564 - acc: 0.4564 - val_loss: 1.6619 - val_acc: 0.4893\n",
            "Epoch 2/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 1.2591 - acc: 0.6025 - val_loss: 1.3209 - val_acc: 0.5922\n",
            "Epoch 3/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 1.1068 - acc: 0.6557 - val_loss: 1.1781 - val_acc: 0.6409\n",
            "Epoch 4/200\n",
            "391/391 [==============================] - 64s 164ms/step - loss: 1.0025 - acc: 0.6958 - val_loss: 1.0170 - val_acc: 0.7056\n",
            "Epoch 5/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.9326 - acc: 0.7241 - val_loss: 0.9915 - val_acc: 0.7114\n",
            "Epoch 6/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.8764 - acc: 0.7447 - val_loss: 1.0307 - val_acc: 0.7123\n",
            "Epoch 7/200\n",
            "391/391 [==============================] - 64s 164ms/step - loss: 0.8392 - acc: 0.7585 - val_loss: 0.9814 - val_acc: 0.7190\n",
            "Epoch 8/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.8044 - acc: 0.7739 - val_loss: 0.8292 - val_acc: 0.7669\n",
            "Epoch 9/200\n",
            "391/391 [==============================] - 64s 164ms/step - loss: 0.7819 - acc: 0.7822 - val_loss: 0.7977 - val_acc: 0.7790\n",
            "Epoch 10/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.7551 - acc: 0.7907 - val_loss: 0.9201 - val_acc: 0.7535\n",
            "Epoch 11/200\n",
            "391/391 [==============================] - 64s 164ms/step - loss: 0.7407 - acc: 0.7987 - val_loss: 0.9769 - val_acc: 0.7373\n",
            "Epoch 12/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.7239 - acc: 0.8037 - val_loss: 0.8925 - val_acc: 0.7591\n",
            "Epoch 13/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.7076 - acc: 0.8097 - val_loss: 0.9320 - val_acc: 0.7591\n",
            "Epoch 14/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6983 - acc: 0.8148 - val_loss: 0.7591 - val_acc: 0.8018\n",
            "Epoch 15/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6842 - acc: 0.8189 - val_loss: 0.7343 - val_acc: 0.8127\n",
            "Epoch 16/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6785 - acc: 0.8235 - val_loss: 0.9198 - val_acc: 0.7676\n",
            "Epoch 17/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6662 - acc: 0.8271 - val_loss: 0.9699 - val_acc: 0.7494\n",
            "Epoch 18/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6617 - acc: 0.8305 - val_loss: 0.8371 - val_acc: 0.7918\n",
            "Epoch 19/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6575 - acc: 0.8337 - val_loss: 0.6873 - val_acc: 0.8299\n",
            "Epoch 20/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.6444 - acc: 0.8367 - val_loss: 0.7650 - val_acc: 0.8057\n",
            "Epoch 21/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.6459 - acc: 0.8387 - val_loss: 0.8081 - val_acc: 0.7997\n",
            "Epoch 22/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.6334 - acc: 0.8435 - val_loss: 0.8360 - val_acc: 0.7934\n",
            "Epoch 23/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6294 - acc: 0.8448 - val_loss: 0.6984 - val_acc: 0.8336\n",
            "Epoch 24/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6325 - acc: 0.8441 - val_loss: 0.7642 - val_acc: 0.8110\n",
            "Epoch 25/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6247 - acc: 0.8473 - val_loss: 0.8419 - val_acc: 0.7933\n",
            "Epoch 26/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6194 - acc: 0.8515 - val_loss: 0.8820 - val_acc: 0.7857\n",
            "Epoch 27/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6186 - acc: 0.8513 - val_loss: 0.7527 - val_acc: 0.8250\n",
            "Epoch 28/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6101 - acc: 0.8564 - val_loss: 0.7999 - val_acc: 0.8122\n",
            "Epoch 29/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6101 - acc: 0.8529 - val_loss: 0.7751 - val_acc: 0.8097\n",
            "Epoch 30/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6093 - acc: 0.8566 - val_loss: 0.7485 - val_acc: 0.8216\n",
            "Epoch 31/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.6076 - acc: 0.8578 - val_loss: 0.8433 - val_acc: 0.8013\n",
            "Epoch 32/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.6008 - acc: 0.8616 - val_loss: 0.7863 - val_acc: 0.8202\n",
            "Epoch 33/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.6048 - acc: 0.8616 - val_loss: 0.7310 - val_acc: 0.8322\n",
            "Epoch 34/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.5990 - acc: 0.8624 - val_loss: 0.7387 - val_acc: 0.8345\n",
            "Epoch 35/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.6034 - acc: 0.8619 - val_loss: 0.8024 - val_acc: 0.8138\n",
            "Epoch 36/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5988 - acc: 0.8646 - val_loss: 0.8333 - val_acc: 0.8071\n",
            "Epoch 37/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5961 - acc: 0.8670 - val_loss: 0.8204 - val_acc: 0.8164\n",
            "Epoch 38/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.5970 - acc: 0.8661 - val_loss: 0.7904 - val_acc: 0.8238\n",
            "Epoch 39/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.5959 - acc: 0.8684 - val_loss: 0.8686 - val_acc: 0.8050\n",
            "Epoch 40/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.5977 - acc: 0.8680 - val_loss: 0.7975 - val_acc: 0.8274\n",
            "Epoch 41/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.5932 - acc: 0.8695 - val_loss: 0.8363 - val_acc: 0.8144\n",
            "Epoch 42/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5897 - acc: 0.8703 - val_loss: 0.9304 - val_acc: 0.7953\n",
            "Epoch 43/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5840 - acc: 0.8736 - val_loss: 0.7934 - val_acc: 0.8232\n",
            "Epoch 44/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5920 - acc: 0.8701 - val_loss: 0.6787 - val_acc: 0.8565\n",
            "Epoch 45/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.5914 - acc: 0.8691 - val_loss: 0.7369 - val_acc: 0.8358\n",
            "Epoch 46/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5842 - acc: 0.8742 - val_loss: 0.7830 - val_acc: 0.8264\n",
            "Epoch 47/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5861 - acc: 0.8746 - val_loss: 0.7775 - val_acc: 0.8257\n",
            "Epoch 48/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5878 - acc: 0.8745 - val_loss: 0.6269 - val_acc: 0.8695\n",
            "Epoch 49/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5868 - acc: 0.8747 - val_loss: 0.8707 - val_acc: 0.8113\n",
            "Epoch 50/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5867 - acc: 0.8742 - val_loss: 0.9506 - val_acc: 0.7821\n",
            "Epoch 51/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5882 - acc: 0.8758 - val_loss: 0.6789 - val_acc: 0.8532\n",
            "Epoch 52/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5822 - acc: 0.8782 - val_loss: 0.7727 - val_acc: 0.8302\n",
            "Epoch 53/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5791 - acc: 0.8776 - val_loss: 0.8388 - val_acc: 0.8170\n",
            "Epoch 54/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5849 - acc: 0.8782 - val_loss: 0.7569 - val_acc: 0.8411\n",
            "Epoch 55/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5817 - acc: 0.8785 - val_loss: 1.0168 - val_acc: 0.7820\n",
            "Epoch 56/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5818 - acc: 0.8810 - val_loss: 0.7758 - val_acc: 0.8303\n",
            "Epoch 57/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5841 - acc: 0.8789 - val_loss: 0.8311 - val_acc: 0.8192\n",
            "Epoch 58/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5736 - acc: 0.8829 - val_loss: 0.7448 - val_acc: 0.8407\n",
            "Epoch 59/200\n",
            "391/391 [==============================] - 64s 164ms/step - loss: 0.5814 - acc: 0.8793 - val_loss: 0.8091 - val_acc: 0.8268\n",
            "Epoch 60/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5753 - acc: 0.8817 - val_loss: 0.7780 - val_acc: 0.8430\n",
            "Epoch 61/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.5760 - acc: 0.8839 - val_loss: 0.7201 - val_acc: 0.8484\n",
            "Epoch 62/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.5058 - acc: 0.9076 - val_loss: 0.5922 - val_acc: 0.8900\n",
            "Epoch 63/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4718 - acc: 0.9193 - val_loss: 0.5840 - val_acc: 0.8915\n",
            "Epoch 64/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4550 - acc: 0.9240 - val_loss: 0.5766 - val_acc: 0.8938\n",
            "Epoch 65/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4443 - acc: 0.9258 - val_loss: 0.6285 - val_acc: 0.8823\n",
            "Epoch 66/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4363 - acc: 0.9273 - val_loss: 0.5967 - val_acc: 0.8879\n",
            "Epoch 67/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4318 - acc: 0.9281 - val_loss: 0.5949 - val_acc: 0.8854\n",
            "Epoch 68/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4200 - acc: 0.9301 - val_loss: 0.5690 - val_acc: 0.8929\n",
            "Epoch 69/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4175 - acc: 0.9309 - val_loss: 0.5882 - val_acc: 0.8884\n",
            "Epoch 70/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4098 - acc: 0.9322 - val_loss: 0.5518 - val_acc: 0.8994\n",
            "Epoch 71/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.4069 - acc: 0.9333 - val_loss: 0.5667 - val_acc: 0.8957\n",
            "Epoch 72/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.4003 - acc: 0.9349 - val_loss: 0.5804 - val_acc: 0.8914\n",
            "Epoch 73/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.3956 - acc: 0.9347 - val_loss: 0.5627 - val_acc: 0.8958\n",
            "Epoch 74/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3891 - acc: 0.9375 - val_loss: 0.5671 - val_acc: 0.8932\n",
            "Epoch 75/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3897 - acc: 0.9353 - val_loss: 0.5655 - val_acc: 0.8953\n",
            "Epoch 76/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3872 - acc: 0.9363 - val_loss: 0.5614 - val_acc: 0.8942\n",
            "Epoch 77/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.3836 - acc: 0.9368 - val_loss: 0.5856 - val_acc: 0.8896\n",
            "Epoch 78/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.3751 - acc: 0.9384 - val_loss: 0.5678 - val_acc: 0.8904\n",
            "Epoch 79/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.3755 - acc: 0.9374 - val_loss: 0.5393 - val_acc: 0.8976\n",
            "Epoch 80/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.3688 - acc: 0.9397 - val_loss: 0.6032 - val_acc: 0.8828\n",
            "Epoch 81/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3691 - acc: 0.9384 - val_loss: 0.5713 - val_acc: 0.8894\n",
            "Epoch 82/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3613 - acc: 0.9402 - val_loss: 0.6095 - val_acc: 0.8798\n",
            "Epoch 83/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3580 - acc: 0.9420 - val_loss: 0.5913 - val_acc: 0.8858\n",
            "Epoch 84/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3578 - acc: 0.9399 - val_loss: 0.5568 - val_acc: 0.8915\n",
            "Epoch 85/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.3559 - acc: 0.9418 - val_loss: 0.5641 - val_acc: 0.8905\n",
            "Epoch 86/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.3530 - acc: 0.9408 - val_loss: 0.5470 - val_acc: 0.8951\n",
            "Epoch 87/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3484 - acc: 0.9409 - val_loss: 0.5608 - val_acc: 0.8894\n",
            "Epoch 88/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3476 - acc: 0.9409 - val_loss: 0.5417 - val_acc: 0.8942\n",
            "Epoch 89/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.3444 - acc: 0.9424 - val_loss: 0.5213 - val_acc: 0.8983\n",
            "Epoch 90/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3417 - acc: 0.9411 - val_loss: 0.5593 - val_acc: 0.8905\n",
            "Epoch 91/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.3402 - acc: 0.9425 - val_loss: 0.5547 - val_acc: 0.8913\n",
            "Epoch 92/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.3374 - acc: 0.9439 - val_loss: 0.5193 - val_acc: 0.8986\n",
            "Epoch 93/200\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.3344 - acc: 0.9436 - val_loss: 0.5343 - val_acc: 0.8945\n",
            "Epoch 94/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3335 - acc: 0.9422 - val_loss: 0.5869 - val_acc: 0.8825\n",
            "Epoch 95/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.3363 - acc: 0.9409 - val_loss: 0.5370 - val_acc: 0.8914\n",
            "Epoch 96/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3312 - acc: 0.9423 - val_loss: 0.5611 - val_acc: 0.8873\n",
            "Epoch 97/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3301 - acc: 0.9433 - val_loss: 0.5731 - val_acc: 0.8873\n",
            "Epoch 98/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3284 - acc: 0.9428 - val_loss: 0.5210 - val_acc: 0.8958\n",
            "Epoch 99/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3255 - acc: 0.9428 - val_loss: 0.5192 - val_acc: 0.8965\n",
            "Epoch 100/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3236 - acc: 0.9449 - val_loss: 0.6045 - val_acc: 0.8787\n",
            "Epoch 101/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3176 - acc: 0.9454 - val_loss: 0.5432 - val_acc: 0.8943\n",
            "Epoch 102/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3178 - acc: 0.9444 - val_loss: 0.5553 - val_acc: 0.8887\n",
            "Epoch 103/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3169 - acc: 0.9454 - val_loss: 0.5850 - val_acc: 0.8827\n",
            "Epoch 104/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3213 - acc: 0.9439 - val_loss: 0.5320 - val_acc: 0.8944\n",
            "Epoch 105/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3182 - acc: 0.9445 - val_loss: 0.5256 - val_acc: 0.8931\n",
            "Epoch 106/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3152 - acc: 0.9449 - val_loss: 0.5878 - val_acc: 0.8811\n",
            "Epoch 107/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3161 - acc: 0.9439 - val_loss: 0.6309 - val_acc: 0.8711\n",
            "Epoch 108/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3117 - acc: 0.9453 - val_loss: 0.5659 - val_acc: 0.8889\n",
            "Epoch 109/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3122 - acc: 0.9448 - val_loss: 0.5522 - val_acc: 0.8898\n",
            "Epoch 110/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3166 - acc: 0.9427 - val_loss: 0.5507 - val_acc: 0.8919\n",
            "Epoch 111/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3140 - acc: 0.9445 - val_loss: 0.5675 - val_acc: 0.8834\n",
            "Epoch 112/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3021 - acc: 0.9472 - val_loss: 0.5556 - val_acc: 0.8894\n",
            "Epoch 113/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3121 - acc: 0.9441 - val_loss: 0.5019 - val_acc: 0.8984\n",
            "Epoch 114/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3065 - acc: 0.9451 - val_loss: 0.5331 - val_acc: 0.8916\n",
            "Epoch 115/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3067 - acc: 0.9456 - val_loss: 0.5648 - val_acc: 0.8842\n",
            "Epoch 116/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3097 - acc: 0.9438 - val_loss: 0.5897 - val_acc: 0.8823\n",
            "Epoch 117/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3048 - acc: 0.9454 - val_loss: 0.5053 - val_acc: 0.8940\n",
            "Epoch 118/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3045 - acc: 0.9440 - val_loss: 0.5519 - val_acc: 0.8905\n",
            "Epoch 119/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3035 - acc: 0.9458 - val_loss: 0.5998 - val_acc: 0.8771\n",
            "Epoch 120/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3050 - acc: 0.9448 - val_loss: 0.5480 - val_acc: 0.8888\n",
            "Epoch 121/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.3003 - acc: 0.9455 - val_loss: 0.5808 - val_acc: 0.8808\n",
            "Epoch 122/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2783 - acc: 0.9546 - val_loss: 0.4953 - val_acc: 0.9029\n",
            "Epoch 123/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2646 - acc: 0.9596 - val_loss: 0.4845 - val_acc: 0.9046\n",
            "Epoch 124/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2589 - acc: 0.9616 - val_loss: 0.4886 - val_acc: 0.9057\n",
            "Epoch 125/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2540 - acc: 0.9633 - val_loss: 0.4891 - val_acc: 0.9057\n",
            "Epoch 126/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2541 - acc: 0.9629 - val_loss: 0.4891 - val_acc: 0.9053\n",
            "Epoch 127/200\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.2517 - acc: 0.9637 - val_loss: 0.4924 - val_acc: 0.9043\n",
            "Epoch 128/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2452 - acc: 0.9661 - val_loss: 0.4970 - val_acc: 0.9048\n",
            "Epoch 129/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2480 - acc: 0.9647 - val_loss: 0.4770 - val_acc: 0.9075\n",
            "Epoch 130/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2442 - acc: 0.9658 - val_loss: 0.4956 - val_acc: 0.9046\n",
            "Epoch 131/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2419 - acc: 0.9670 - val_loss: 0.5013 - val_acc: 0.9032\n",
            "Epoch 132/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2393 - acc: 0.9670 - val_loss: 0.4973 - val_acc: 0.9044\n",
            "Epoch 133/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2440 - acc: 0.9655 - val_loss: 0.4938 - val_acc: 0.9064\n",
            "Epoch 134/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2399 - acc: 0.9666 - val_loss: 0.5029 - val_acc: 0.9032\n",
            "Epoch 135/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2394 - acc: 0.9661 - val_loss: 0.5012 - val_acc: 0.9025\n",
            "Epoch 136/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2364 - acc: 0.9684 - val_loss: 0.5038 - val_acc: 0.9030\n",
            "Epoch 137/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2352 - acc: 0.9685 - val_loss: 0.4998 - val_acc: 0.9043\n",
            "Epoch 138/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2356 - acc: 0.9681 - val_loss: 0.4931 - val_acc: 0.9046\n",
            "Epoch 139/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2364 - acc: 0.9673 - val_loss: 0.4898 - val_acc: 0.9059\n",
            "Epoch 140/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2336 - acc: 0.9693 - val_loss: 0.4989 - val_acc: 0.9047\n",
            "Epoch 141/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2331 - acc: 0.9686 - val_loss: 0.4817 - val_acc: 0.9075\n",
            "Epoch 142/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2279 - acc: 0.9713 - val_loss: 0.5086 - val_acc: 0.9015\n",
            "Epoch 143/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2325 - acc: 0.9681 - val_loss: 0.4941 - val_acc: 0.9046\n",
            "Epoch 144/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2320 - acc: 0.9676 - val_loss: 0.4893 - val_acc: 0.9067\n",
            "Epoch 145/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2320 - acc: 0.9685 - val_loss: 0.4882 - val_acc: 0.9052\n",
            "Epoch 146/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2274 - acc: 0.9703 - val_loss: 0.5043 - val_acc: 0.9021\n",
            "Epoch 147/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2253 - acc: 0.9703 - val_loss: 0.4911 - val_acc: 0.9046\n",
            "Epoch 148/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2266 - acc: 0.9695 - val_loss: 0.4809 - val_acc: 0.9073\n",
            "Epoch 149/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2259 - acc: 0.9704 - val_loss: 0.4915 - val_acc: 0.9044\n",
            "Epoch 150/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2244 - acc: 0.9716 - val_loss: 0.4892 - val_acc: 0.9060\n",
            "Epoch 151/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2228 - acc: 0.9716 - val_loss: 0.4976 - val_acc: 0.9046\n",
            "Epoch 152/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2240 - acc: 0.9703 - val_loss: 0.4854 - val_acc: 0.9056\n",
            "Epoch 153/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2231 - acc: 0.9710 - val_loss: 0.4954 - val_acc: 0.9045\n",
            "Epoch 154/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2218 - acc: 0.9701 - val_loss: 0.4830 - val_acc: 0.9080\n",
            "Epoch 155/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2224 - acc: 0.9707 - val_loss: 0.4967 - val_acc: 0.9079\n",
            "Epoch 156/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2218 - acc: 0.9700 - val_loss: 0.4720 - val_acc: 0.9099\n",
            "Epoch 157/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2196 - acc: 0.9706 - val_loss: 0.5116 - val_acc: 0.9025\n",
            "Epoch 158/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2197 - acc: 0.9717 - val_loss: 0.5060 - val_acc: 0.9021\n",
            "Epoch 159/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2168 - acc: 0.9718 - val_loss: 0.4829 - val_acc: 0.9079\n",
            "Epoch 160/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2165 - acc: 0.9720 - val_loss: 0.4889 - val_acc: 0.9052\n",
            "Epoch 161/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2188 - acc: 0.9711 - val_loss: 0.4892 - val_acc: 0.9066\n",
            "Epoch 162/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2125 - acc: 0.9726 - val_loss: 0.4860 - val_acc: 0.9060\n",
            "Epoch 163/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2107 - acc: 0.9744 - val_loss: 0.4878 - val_acc: 0.9061\n",
            "Epoch 164/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2099 - acc: 0.9743 - val_loss: 0.4852 - val_acc: 0.9069\n",
            "Epoch 165/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2113 - acc: 0.9737 - val_loss: 0.4847 - val_acc: 0.9066\n",
            "Epoch 166/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2081 - acc: 0.9747 - val_loss: 0.4894 - val_acc: 0.9052\n",
            "Epoch 167/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2083 - acc: 0.9754 - val_loss: 0.4885 - val_acc: 0.9060\n",
            "Epoch 168/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2106 - acc: 0.9738 - val_loss: 0.4853 - val_acc: 0.9068\n",
            "Epoch 169/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2084 - acc: 0.9749 - val_loss: 0.4868 - val_acc: 0.9072\n",
            "Epoch 170/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2097 - acc: 0.9742 - val_loss: 0.4852 - val_acc: 0.9078\n",
            "Epoch 171/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2063 - acc: 0.9764 - val_loss: 0.4877 - val_acc: 0.9068\n",
            "Epoch 172/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2075 - acc: 0.9751 - val_loss: 0.4895 - val_acc: 0.9073\n",
            "Epoch 173/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2071 - acc: 0.9753 - val_loss: 0.4891 - val_acc: 0.9072\n",
            "Epoch 174/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2054 - acc: 0.9761 - val_loss: 0.4846 - val_acc: 0.9062\n",
            "Epoch 175/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2052 - acc: 0.9767 - val_loss: 0.4796 - val_acc: 0.9083\n",
            "Epoch 176/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2047 - acc: 0.9762 - val_loss: 0.4816 - val_acc: 0.9088\n",
            "Epoch 177/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2068 - acc: 0.9756 - val_loss: 0.4818 - val_acc: 0.9081\n",
            "Epoch 178/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2059 - acc: 0.9748 - val_loss: 0.4823 - val_acc: 0.9075\n",
            "Epoch 179/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2062 - acc: 0.9755 - val_loss: 0.4892 - val_acc: 0.9054\n",
            "Epoch 180/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2049 - acc: 0.9759 - val_loss: 0.4841 - val_acc: 0.9067\n",
            "Epoch 181/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2052 - acc: 0.9752 - val_loss: 0.4845 - val_acc: 0.9074\n",
            "Epoch 182/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2030 - acc: 0.9764 - val_loss: 0.4812 - val_acc: 0.9079\n",
            "Epoch 183/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2036 - acc: 0.9759 - val_loss: 0.4844 - val_acc: 0.9073\n",
            "Epoch 184/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2043 - acc: 0.9763 - val_loss: 0.4841 - val_acc: 0.9083\n",
            "Epoch 185/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2032 - acc: 0.9767 - val_loss: 0.4845 - val_acc: 0.9068\n",
            "Epoch 186/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2038 - acc: 0.9756 - val_loss: 0.4875 - val_acc: 0.9066\n",
            "Epoch 187/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2055 - acc: 0.9749 - val_loss: 0.4848 - val_acc: 0.9082\n",
            "Epoch 188/200\n",
            "391/391 [==============================] - 63s 162ms/step - loss: 0.2034 - acc: 0.9759 - val_loss: 0.4865 - val_acc: 0.9059\n",
            "Epoch 189/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2041 - acc: 0.9752 - val_loss: 0.4803 - val_acc: 0.9079\n",
            "Epoch 190/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2043 - acc: 0.9762 - val_loss: 0.4837 - val_acc: 0.9063\n",
            "Epoch 191/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2019 - acc: 0.9769 - val_loss: 0.4799 - val_acc: 0.9067\n",
            "Epoch 192/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2033 - acc: 0.9764 - val_loss: 0.4794 - val_acc: 0.9086\n",
            "Epoch 193/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2019 - acc: 0.9766 - val_loss: 0.4812 - val_acc: 0.9084\n",
            "Epoch 194/200\n",
            "391/391 [==============================] - 63s 160ms/step - loss: 0.2042 - acc: 0.9747 - val_loss: 0.4799 - val_acc: 0.9083\n",
            "Epoch 195/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2013 - acc: 0.9764 - val_loss: 0.4784 - val_acc: 0.9072\n",
            "Epoch 196/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2017 - acc: 0.9767 - val_loss: 0.4789 - val_acc: 0.9073\n",
            "Epoch 197/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.1998 - acc: 0.9772 - val_loss: 0.4791 - val_acc: 0.9075\n",
            "Epoch 198/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2017 - acc: 0.9769 - val_loss: 0.4787 - val_acc: 0.9079\n",
            "Epoch 199/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2017 - acc: 0.9764 - val_loss: 0.4818 - val_acc: 0.9071\n",
            "Epoch 200/200\n",
            "391/391 [==============================] - 63s 161ms/step - loss: 0.2004 - acc: 0.9774 - val_loss: 0.4831 - val_acc: 0.9075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gqiZvG9iDbAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12255b05-45a6-4119-b9c7-c9ebec0e3e52"
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import h5py\n",
        "f = h5py.File('nin_bn.h5', 'r')\n",
        "print(f)\n",
        "# dset = f['loss']\n",
        "# data = np.array(dset[:,:,:])\n",
        "# file = 'test.jpg'\n",
        "# cv2.imwrite(file, data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<HDF5 file \"nin_bn.h5\" (mode r)>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}